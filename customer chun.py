# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wKv-xdEqEvG9A4j6OpDZbYKM87SeXKfS

Data loading
"""

import pandas as pd

try:
    df_customer_churn = pd.read_csv('customer_churn_dataset.csv')
    display(df_customer_churn.head())
    print(df_customer_churn.shape)
except FileNotFoundError:
    print("Error: 'customer_churn_dataset.csv' not found. Please ensure the file is in the correct location and accessible.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

"""## Data exploration

"""

print("Data Types:")
print(df_customer_churn.dtypes)


print("\nMissing Values:")
missing_values = df_customer_churn.isnull().sum()
missing_percentage = (missing_values / len(df_customer_churn)) * 100
print(pd.concat([missing_values, missing_percentage], axis=1, keys=['Count', '%']))


numerical_cols = df_customer_churn.select_dtypes(include=['number']).columns
print("\nDescriptive Statistics for Numerical Features:")
print(df_customer_churn[numerical_cols].describe())

import matplotlib.pyplot as plt

df_customer_churn[numerical_cols].hist(figsize=(10, 8))
plt.suptitle("Histograms of Numerical Features", fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()


categorical_cols = df_customer_churn.select_dtypes(include=['object']).columns
print("\nFrequency Counts for Categorical Features:")
for col in categorical_cols:
    print(f"\n{col}:")
    print(df_customer_churn[col].value_counts())


for col in categorical_cols:
    df_customer_churn[col].value_counts().plot(kind='bar', figsize=(8, 6))
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()


print("\nChurn Rate by Categorical Feature:")
for col in categorical_cols:
    print(f"\n{col}:")
    print(df_customer_churn.groupby(col)['churn'].mean())


for col in numerical_cols:
    plt.figure(figsize=(8, 6))
    df_customer_churn.boxplot(column=col, by='churn')
    plt.title(f'{col} vs. Churn')
    plt.suptitle('')
    plt.show()


print("\nPotential Outliers (Visualized in Boxplots above):")

print("\nSummary (To be added in markdown format)")

"""## Data cleaning


"""

import numpy as np

def cap_outliers(series, method='iqr', factor=1.5):
    if method == 'iqr':
        q1 = series.quantile(0.25)
        q3 = series.quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - factor * iqr
        upper_bound = q3 + factor * iqr
        return np.clip(series, lower_bound, upper_bound)
    else:
        mean = series.mean()
        std = series.std()
        lower_bound = mean - factor * std
        upper_bound = mean + factor * std
        return np.clip(series, lower_bound, upper_bound)

for col in ['tenure_months', 'total_charges', 'monthly_charges']:
    df_customer_churn[col] = cap_outliers(df_customer_churn[col])

df_customer_churn['total_charges'] = pd.to_numeric(df_customer_churn['total_charges'], errors='coerce')

num_duplicates = df_customer_churn.duplicated().sum()
df_customer_churn = df_customer_churn.drop_duplicates()
print(f"Number of duplicate rows removed: {num_duplicates}")

print("\nDescriptive Statistics for Numerical Features (After Cleaning):")
print(df_customer_churn[numerical_cols].describe())

df_customer_churn[numerical_cols].hist(figsize=(10, 8))
plt.suptitle("Histograms of Numerical Features (After Cleaning)", fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

display(df_customer_churn.head())
print(f"Shape of cleaned dataframe: {df_customer_churn.shape}")

"""## Data preparation


"""

from sklearn.preprocessing import OneHotEncoder, StandardScaler

categorical_cols = ['contract_type', 'payment_method', 'internet_service', 'tech_support', 'streaming_tv']
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_features = encoder.fit_transform(df_customer_churn[categorical_cols])
encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_cols))
df_customer_churn = pd.concat([df_customer_churn, encoded_df], axis=1)

numerical_cols = ['tenure_months', 'monthly_charges', 'total_charges']
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df_customer_churn[numerical_cols])
df_customer_churn[numerical_cols] = scaled_features

display(df_customer_churn.head())

"""## Data splitting


"""

from sklearn.model_selection import train_test_split

X = df_customer_churn.drop('churn', axis=1)
y = df_customer_churn['churn']

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)

X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_val shape:", X_val.shape)
print("y_val shape:", y_val.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

"""## Model training

**Reasoning**:
Train a Logistic Regression model and a Random Forest Classifier using the training data.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

logreg_model = LogisticRegression(max_iter=1000)
rf_model = RandomForestClassifier()

try:
    logreg_model.fit(X_train, y_train)
    rf_model.fit(X_train, y_train)
    print("Models trained successfully.")
except Exception as e:
    print(f"An error occurred during model training: {e}")

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

X_train = X_train.drop('customer_id', axis=1)
X_val = X_val.drop('customer_id', axis=1)
X_test = X_test.drop('customer_id', axis=1)

logreg_model = LogisticRegression(max_iter=1000)
rf_model = RandomForestClassifier()

try:
    logreg_model.fit(X_train, y_train)
    rf_model.fit(X_train, y_train)
    print("Models trained successfully.")
except Exception as e:
    print(f"An error occurred during model training: {e}")

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

categorical_cols_to_drop = ['contract_type', 'payment_method', 'internet_service', 'tech_support', 'streaming_tv']

X_train = X_train.drop(columns=categorical_cols_to_drop, errors='ignore')
X_val = X_val.drop(columns=categorical_cols_to_drop, errors='ignore')
X_test = X_test.drop(columns=categorical_cols_to_drop, errors='ignore')


logreg_model = LogisticRegression(max_iter=1000)
rf_model = RandomForestClassifier()

try:
    logreg_model.fit(X_train, y_train)
    rf_model.fit(X_train, y_train)
    print("Models trained successfully.")
except Exception as e:
    print(f"An error occurred during model training: {e}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

logreg_pred = logreg_model.predict(X_val)
rf_pred = rf_model.predict(X_val)

logreg_probs = logreg_model.predict_proba(X_val)[:, 1]
rf_probs = rf_model.predict_proba(X_val)[:, 1]

logreg_accuracy = accuracy_score(y_val, logreg_pred)
rf_accuracy = accuracy_score(y_val, rf_pred)

logreg_precision = precision_score(y_val, logreg_pred, average='weighted')
rf_precision = precision_score(y_val, rf_pred, average='weighted')

logreg_recall = recall_score(y_val, logreg_pred, average='weighted')
rf_recall = recall_score(y_val, rf_pred, average='weighted')

logreg_f1 = f1_score(y_val, logreg_pred, average='weighted')
rf_f1 = f1_score(y_val, rf_pred, average='weighted')

logreg_auc = roc_auc_score(y_val, logreg_probs)
rf_auc = roc_auc_score(y_val, rf_probs)

print("| Metric          | Logistic Regression | Random Forest      |")
print("|-----------------|----------------------|---------------------|")
print(f"| Accuracy        | {logreg_accuracy:.4f}                  | {rf_accuracy:.4f}                 |")
print(f"| Precision       | {logreg_precision:.4f}                  | {rf_precision:.4f}                 |")
print(f"| Recall          | {logreg_recall:.4f}                  | {rf_recall:.4f}                 |")
print(f"| F1-score        | {logreg_f1:.4f}                  | {rf_f1:.4f}                 |")
print(f"| AUC-ROC         | {logreg_auc:.4f}                  | {rf_auc:.4f}                 |")

if rf_auc > logreg_auc:
    print("\nRandom Forest performs better based on AUC-ROC.")
else:
    print("\nLogistic Regression performs better based on AUC-ROC.")